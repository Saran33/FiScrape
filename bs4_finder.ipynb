{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "from unicodedata import normalize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.businessinsider.com/author/francis-agustin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rq.get(url)\n",
    "page_source = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = page_source.find('p', attrs={'data-test': \"author-aside-title\"}).text.strip()\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_bio = [a.text.strip() for a in page_source.select('div.col-12.col-md-8.author-description > p')]\n",
    "author_bio = normalize(\"NFKD\", ''.join(map(str, author_bio)).replace('  ', ' ').strip())\n",
    "author_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'a.author-contact-icon-link.share-link.email::attr(href)'\n",
    "\n",
    "author_email = page_source.find('a', attrs={'class': 'author-contact-icon-link share-link email'})['href'].replace('mailto:', '').strip()\n",
    "author_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_twitter = page_source.find('a', attrs={\n",
    "                    'class': 'author-contact-icon-link share-link twitter'})['href'].strip()\n",
    "author_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://markets.businessinsider.com/news/currencies/stablecoin-regulation-cryptocurrency-banks-biden-treasury-bitcoin-circle-tether-binance-2021-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rq.get(url)\n",
    "page_source = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_articles(text):\n",
    "    # strip the unicode articles\n",
    "    #text = normalize(\"NFKD\", text.strip(u'\\u201c'u'\\u201d'))\n",
    "    text = normalize(\"NFKD\", ''.join(map(str, text)).replace('  ', ' ').strip())\n",
    "    return text\n",
    "\n",
    "from itemloaders.processors import Join, MapCompose\n",
    "from scrapy.item import Item, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = MapCompose(remove_articles)\n",
    "\n",
    "class InsiderArtItem(Item):\n",
    "    content = Field(\n",
    "        input_processor=MapCompose(remove_articles),\n",
    "        # TakeFirst return the first value not the whole list\n",
    "        output_processor=Join()\n",
    "        )\n",
    "\n",
    "article = InsiderArtItem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = [a.text.strip() for a in page_source.select(\n",
    "    'div.col-xs-12.news-content.no-padding > p, div.col-xs-12.news-content.no-padding > ul > li[aria-level*=\"1\"]')] # div.col-xs-12.news-content.no-padding > p > a,\n",
    "article_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article['content'] = article_content\n",
    "# article['content']\n",
    "\n",
    "# article['content'] = [remove_articles(line) for line in article_content]\n",
    "# article['content'] = ' '.join(article['content'])\n",
    "# article['content']\n",
    "\n",
    "article_content = [remove_articles(a.text) for a in page_source.select(\n",
    "    'div.col-xs-12.news-content.no-padding > p, div.col-xs-12.news-content.no-padding > ul > li[aria-level*=\"1\"]')] # div.col-xs-12.news-content.no-padding > p > a,\n",
    "article_content = ''.join(article_content)\n",
    "article_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# import requests as rq\n",
    "import grequests\n",
    "from unicodedata import normalize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls =  ['https://www.cnbc.com/matthew-j-belvedere/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(bio_links):\n",
    "    reqs = [grequests.get(bio_link) for bio_link in bio_links]\n",
    "    resps = grequests.map(reqs)\n",
    "    return resps\n",
    "\n",
    "def process_author(resps):\n",
    "    auth  = 'Matthew J. Belvedere'\n",
    "    authors = []\n",
    "    authors = {}\n",
    "    authors[f'{auth}'] = {}\n",
    "    for (auth, response) in zip(authors.keys(), resps):\n",
    "        page_source = BeautifulSoup(response.text, 'lxml')\n",
    "        return page_source\n",
    "        # pos = page_source.find('span', attrs={'class': \"RenderBioDetails-jobTitle\"}).text.strip()\n",
    "        # print(pos)\n",
    "        # authors[f'{auth}']['author_position'] = pos\n",
    "        # return page_source, pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = get_urls(urls)\n",
    "page_source  = process_author(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_script = page_source.xpath('//script[contains(., \"locationBeforeTransitions\")]/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import requests\n",
    "from FiScrape.items import index_of_nth\n",
    "import json\n",
    "  \n",
    "  \n",
    "URL = \"https://www.cnbc.com/matthew-j-belvedere/\"\n",
    "  \n",
    "HEADERS = ({'User-Agent':\n",
    "            \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; \\\n",
    "            +http://www.google.com/bot.html) Chrome/W.X.Y.Z Safari/537.36\",\\\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "            \n",
    "webpage = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "dom = etree.HTML(str(soup))\n",
    "js_script = dom.xpath('//script[contains(., \"locationBeforeTransitions\")]/text()')\n",
    "# print(js_script)\n",
    "\n",
    "txt = js_script[0]\n",
    "start = txt.find('modules', txt.find('modules')+1) +10\n",
    "end = index_of_nth(txt, 'column', 4) - 16\n",
    "json_string = txt[start:end]\n",
    "# print(json_string)\n",
    "data = json.loads(json_string)\n",
    "body = data.get('data').get('body')\n",
    "content = body.get('content')\n",
    "p_tags = content[0].get('children')\n",
    "author_bio = []\n",
    "for i in range(len(p_tags)):\n",
    "    author_bio.append(''.join(map(str, p_tags[i].get('children'))))\n",
    "author_bio = ''.join(author_bio).strip()\n",
    "# print (\"author_bio:\", author_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(author_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import requests\n",
    "from FiScrape.items import index_of_nth\n",
    "import json\n",
    "  \n",
    "  \n",
    "URL = 'https://www.reuters.com/authors/scott-murdoch'\n",
    "# URL = 'https://www.reuters.com/authors/nelson-renteria/'\n",
    "  \n",
    "HEADERS = ({'User-Agent':\n",
    "            \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; \\\n",
    "            +http://www.google.com/bot.html) Chrome/W.X.Y.Z Safari/537.36\",\\\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "            \n",
    "webpage = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "dom = etree.HTML(str(soup))\n",
    "js_script = dom.xpath('//script[contains(., \"window.Fusion\")]/text()')\n",
    "txt = str(js_script)\n",
    "with open('initial_response2.json', 'w') as  f:\n",
    "    f.write(txt)\n",
    "# start = txt.find('description') +len('description')+3\n",
    "# print(start)\n",
    "# end = txt[start:].find('\",', ) +start-1\n",
    "# print(end)\n",
    "# bio_string = txt[start:end]\n",
    "# print(bio_string.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import requests\n",
    "from FiScrape.items import index_of_nth\n",
    "import json\n",
    "  \n",
    "  \n",
    "# URL = 'https://www.reuters.com/authors/scott-murdoch'\n",
    "# URL = 'https://www.reuters.com/authors/nelson-renteria/'\n",
    "# URL = 'https://www.reuters.com/authors/jody-godoy/'\n",
    "# URL = 'https://www.reuters.com/authors/john-mccrank/'\n",
    "URL = 'https://www.reuters.com/authors/r-mark-halligan/'\n",
    "  \n",
    "HEADERS = ({'User-Agent':\n",
    "            \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; \\\n",
    "            +http://www.google.com/bot.html) Chrome/W.X.Y.Z Safari/537.36\",\\\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "            \n",
    "webpage = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "dom = etree.HTML(str(soup))\n",
    "js_script = dom.xpath('//script[contains(., \"window.Fusion\")]/text()')\n",
    "txt = str(js_script)\n",
    "# with open('initial_response2.json', 'w') as  f:\n",
    "#     f.write(txt)\n",
    "start = txt.find('topics') +len('topics')+3\n",
    "print(start)\n",
    "end = txt[start:].find('entity', ) +start-3\n",
    "print(end)\n",
    "author_string = txt[start:end]\n",
    "author_string = author_string.replace(r'\\\\\"', '')\n",
    "print(author_string)\n",
    "json_author = json.loads(author_string)\n",
    "# print(json_author.strip())\n",
    "with open('json_author.json', 'w') as  f:\n",
    "    f.write(json.dumps(json_author))\n",
    "if json_author.get('name') == 'Scott Murdoch':\n",
    "    author_bio = json_author.get('description')\n",
    "    print (author_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "497581ab8b68e3c2ef39c47d7e3c0ff119b32f5944e17571db2f7dc83154eb86"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('pwepip': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
