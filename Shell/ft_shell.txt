
scrapy shell "https://www.ft.com/search?q=bitcoin&sort=date/"

response.xpath("//div[@class='o-teaser o-teaser--article o-teaser--small o-teaser--has-image js-teaser']").get()
snippets = response.xpath("//div[@class='o-teaser o-teaser--article o-teaser--small o-teaser--has-image js-teaser']")
snippets = response.css('div.o-teaser.o-teaser--article.o-teaser--small.o-teaser--has-image.js-teaser')

# published_date
snippets[0].css('time.o-teaser__timestamp-date::datetime').getall()
snippets[0].css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)').getall()
snippets[-1].css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)').getall()
snippets[5].css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)').getall()

snippets[].css('div.o-teaser__heading::attr(text)').getall()
snippets[0].css('div.o-teaser__heading').getall()
snippets[0].css("div.o-teaser__heading").getall()
snippets[0].xpath("//div[@class='o-teaser__heading']/a/text()").getall()
snippets[0].css("div.o-teaser__heading a.js-teaser-heading-link::text").getall()
snippets[5].css("a.js-teaser-heading-link *::text").getall()
headline = snippets[0].css("a.js-teaser-heading-link ::text").getall()
headline = "".join(snippets[0].css("a.js-teaser-heading-link *::text").extract())
headline = "".join(snippets[5].css("a.js-teaser-heading-link *::text").extract())


standfirst = snippets[0].css("p.o-teaser__standfirst ::text").getall()
standfirst = "".join(snippets[0].css("p.o-teaser__standfirst ::text").extract())
standfirst = "".join(snippets[5].css("p.o-teaser__standfirst ::text").extract())

tags = snippets[0].css("a.o-teaser__tag::text").getall()

link = snippets[0].css("div.o-teaser__heading a::attr(href)").get()
url = response.urljoin(next_page[0].extract())

last_date = response.xpath('//div[@class="o-teaser__timestamp time.o-teaser__timestamp-date"]/a/datetime()')[-1].extract()
first_date = snippets[0].css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)').get()
first_date = response.css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)').extract_first()
last_date = snippets[-1].css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)').get()
last_date = response.css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)')[-1].extract()

snippets[0].css('div.o-teaser__timestamp time.o-teaser__timestamp-date::attr(datetime)')

next_page = snippets[0]("a.search-pagination__next-page a::attr(href)").get()
snippets[0].xpath("//a[@class='search-pagination__next-page o-buttons o-buttons--secondary o-buttons-icon o-buttons-icon--arrow-right o-buttons--big o-buttons-icon--icon-only']/@href").getall()
a = snippets[0].xpath("//a[@class='search-pagination__next-page o-buttons o-buttons--secondary o-buttons-icon o-buttons-icon--arrow-right o-buttons--big o-buttons-icon--icon-only']").getall()

snippets[0].follow(a, callback=self.parse)

next_page = response.xpath("//a[@class='search-pagination__next-page o-buttons o-buttons--secondary o-buttons-icon o-buttons-icon--arrow-right o-buttons--big o-buttons-icon--icon-only']/@href")
url = response.urljoin(next_page[0].extract())


article_link = snippets[0].css("div.o-teaser__heading a::attr(href)").get()
article_link = response.urljoin(article_link)
fetch(url[, redirect=True])
article_page = fetch(article_link, redirect=True)
article_page = response.follow(article_link)
article_page = scrapy.Request(article_link)
# or:
scrapy shell 'https://www.ft.com/content/b9f6c876-22bb-498c-a425-78287455a3e1' --nolog
# Multiple athors:
scrapy shell 'https://www.ft.com/content/b26319f6-6cb7-4e0e-a0d9-bac71d9b8c34' --nolog

head = response.css('head').get()
response.css('body').get()
body = response.css('body')
div  = response.css('div').get()

layout = response.css('div.n-layout').getall()
layout = response.css('div.n-layout')
layout.css('p').getall()
layout.css('article').getall()
layout.css('article.site-content').getall()

layout.css('p.article-info__byline').getall()
response.css('p.article-info__byline').getall()
article-info

article_content = response.xpath("//div[@class='article__content']").get()
content = response.xpath("//div[@class='article__content']")
response.xpath('//*[@id="site-content"]/div[3]/div[3]/p[2]/text()')
article_content = response.xpath('//*[@class="article__content-body n-content-body js-article__content-body"]/p/text()')

#CONTENT:
content = response.xpath('.//*/div[@class="article__content-body n-content-body js-article__content-body"]/*/p/text()').getall()
content = response.xpath('//*[contains(@class, "article__content-body n-content-body js-article__content-body")]//text()[not(ancestor::*[@class="n-content-layout__container"])]').getall()
article_content = normalize("NFKD",' '.join(map(str, content)).replace('  ', ' ').strip())
article_content


article_content.getall()

body.css('li.o-header__drawer-menu-item').get()
article = response.css('article.article.article-grid.article-grid--no-full-width-graphics').getall()
content  = body[0].css('.div').getall()

article_footnote = response.xpath('//*[contains(@class, "article__content-body n-content-body js-article__content-body")]/p[1]/em/text()[not(ancestor::*[@class="n-content-layout__container"])]').getall()
article_footnote = response.xpath('//*[contains(@class, "article__content-body n-content-body js-article__content-body")]/p[1]/*[self::em or self::a]//text()[not(ancestor::*[@class="n-content-layout__container"])]').getall()

summary = response.xpath('//*[@class="o-topper__standfirst"]/text()').get()
if response.xpath('//*[@id="site-content"]/div[3]/div[2]/p[last()-1]/em/text()'):
	article_footnote = response.xpath('//*[@id="site-content"]/div[3]/div[2]/p[last()-1]/em/text()').get()
else:
	article_footnote = response.xpath('//*[contains(@class, "article__content-body n-content-body js-article__content-body")]/p[1]/*[self::em or self::a]//text()[not(ancestor::*[@class="n-content-layout__container"])]').getall()

last_paragraph = response.xpath('//*[@id="site-content"]/div[3]/div[3]/p/text()')[-1].get()
last_paragraph = response.xpath('//*[@id="site-content"]/div[3]/div[3]/p[last()-1]/text()').get()
image_caption = response.xpath('//*[@class="n-content-image__caption"]').getall()
image_caption = response.xpath('//*[@id="site-content"]/div[1]/figure/figcaption/text()').get().replace(' ', ' ').strip()
#top_comment = response.xpath('//*[@id="comment-1341de54-ec95-4728-99c1-ff221e44063c"]/div[2]/div/div/div/div/div[2]/div[1]/div[1]/text()')
#top_comment = response.xpath('//*[@id="comment-7df83454-0e93-4352-b8f5-c2f52d8c7cfb"]/div[2]/div/div/div/div/div[2]/div[1]/div[2]/text()')

article = response.xpath("//article")

<h1 class="o-topper__headline o-topper__headline--basic"><span class="article-classifier__gap">Finance industry warns against ‘unnecessarily restrictive’ crypto capital rules</span></h1>

authors = response.css("a.n-content-tag--author")
for author in authors:
	author_name = author.css("a.n-content-tag--author::text").get()
	print (author_name)
	author_url = author.css("a.n-content-tag--author::attr(href)").extract()
	author_url = response.urljoin(''.join(map(str, author_url)))
	print (author_url)

authors = response.xpath('//*[@id="site-content"]/div[3]/div[1]/div/p/a/text()').getall()
authors = response.xpath('//*[@id="site-content"]/div[3]/div[1]/div/p/a[1]/text()').getall()
authors = response.css("a.n-content-tag--author::text").get()

author_urls = response.css("a.n-content-tag--author::attr(href)").getall()
author_urls = response.css("a.n-content-tag--author::attr(href)").get().extract()
author_urls = response.css("a.n-content-tag--author::attr(href)").extract()
for author_url in author_urls:
	url_str = str(author_url)
	url_str = ''.join(map(str, author_url))
	print (type(author_url))
	author_url = response.urljoin(url_str)
	print (author_url)

author_urls = response.css("a.n-content-tag--author::attr(href)").extract()
for author_url in author_urls:
	author_url = response.urljoin(str(author_url))
	print (author_url)

scrapy shell 'https://www.ft.com/laurence-fletcher/' --nolog
author = response.xpath('//div[@class="sub-header sub-header--author"]')
author_name = author.xpath('//h1[@class="sub-header__page-title"]/text()').get().strip()
import time
%timeit author_position = author.css("div.sub-header__strapline::text").get().strip()
%timeit author_position = author.xpath('//div[@class="sub-header__strapline"]/text()').get().strip()
author_bio = author.xpath('/html/body/div[1]/div[2]/div/div[2]/div/div/div/div[3]/div/p/text()').get()
author_bio = author.xpath('//*[@class="sub-header__description"]/descendant::*/text()').get()
author_bio = author.xpath('//*[@class="sub-header__description"]/descendant-or-self::*/text()').getall()
author_bio = author.css('.sub-header__description p ::text').getall()
author_bio = author.css('.sub-header__description p ::text').getall()
author_bio = (' '.join(map(str, author_bio))).replace('  ', ' ').strip()
author_bio

%timeit author_bio = author.xpath('//*[@class="sub-header__description"]/descendant-or-self::*/text()').getall()

email = response.xpath("//a[@class='sub-header__content__link sub-header__content__link--email-address']/@href").get().replace("mailto:", '').strip()
twitter = response.xpath("//a[@class='sub-header__content__link sub-header__content__link--twitter-handle']/@href").get()

scrapy shell 'https://www.ft.com/mohamed-el-erian' --nolog
author = response.xpath('//div[@class="sub-header sub-header--author"]')
author_name = response.xpath('//h1[@class="sub-header__page-title"]/text()').get()
bio = response.xpath('//*[class="sub-header__description"]/p/text()').get()
bio = response.xpath('/html/body/div[1]/div[2]/div/div[2]/div/div/div/div[2]/div/p/text()').get()
author_bio = response.xpath('//*[@class="sub-header__description"]/descendant-or-self::*/text()').getall()
response.xpath('//div[@class="sub-header sub-header--author"]/div[1]/div[@class="o-grid-row"]/div[1]/div[@class="sub-header__content--non-expander"]/div[@class="sub-header__description"]/p').get()
response.xpath('//*[@class="sub-header__description"]/p/text()').get()

scrapy shell 'https://www.ft.com/yasemin-craggs-mersinoglu/' --nolog
name = response.xpath('.//h1[@class="sub-header__page-title"]/text()')

Or statements in Xpath:
# //div[@class='data']/attribute::*[name()='data-property' or name()='data-name']
if response.xpath('//*[@class="sub-header__description"]/p/text()'):
	bio = response.xpath('//*[@class="sub-header__description"]/p/text()')
	check = ("yes")
print ("yes")

LOGIN::
https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com


scrapy crawl ft
scrapy crawl ft -o output.csv -t csv

#Access DB:
sqlite3 FiScrape.db
.tables
.schema article
.schema sentiment
select * from article limit 3;
.quit

BEGIN;
DROP TABLE sentiment;
DROP TABLE temp_sent;
COMMIT;

BEGIN;
SET NOCOUNT ON
CREATE TEMP TABLE temp_sent AS SELECT published_date, id FROM article WHERE article.source_id = (SELECT rowid FROM Source WHERE name = 'ft');
.schema temp_sent
CREATE TEMP TABLE temp_sent2 AS SELECT temp_sent.id, published_date, subjectivity, polarity FROM temp_sent INNER JOIN snip_blob ON snip_blob.article_id = temp_sent.id;
.schema temp_sent2
CREATE TEMP TABLE Sentiment AS SELECT temp_sent2.id, published_date, subjectivity, polarity, compound, negative, neutral, positive FROM temp_sent2 INNER JOIN snip_vader ON snip_vader.article_id = temp_sent2.id;
.schema sentiment
SELECT * FROM sentiment;
COMMIT;
.q


