
scrapy shell --nolog
html_url = 'https://www.zerohedge.com/search-content?qTitleBody=bitcoin&page=0'
fetch(html_url)

js_url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/search-content?qTitleBody=bitcoin&page=0'

fetch(js_url)


TEST::
snippets = response.xpath('//*/div[@class="SearchResult_container__BnK-I"]')
for snippet in snippets:
    foo = snippet.xpath('.//div[@class="SearchResult_authorInfo__33M2f"]//text()').extract()
    print (foo)

response.css('title::text').get()
response.css('h4::text').getall()

import time
%timeit snippets = response.css('div.SearchResult_container__BnK-I')
%timeit 
snippets = response.xpath('//*/div[@class="SearchResult_container__BnK-I"]')

snippets.xpath('.//*[@class="SearchResult_authorInfo__33M2f"]/span[2]/text()').get()
%timeit published_date = snippets.xpath('//*[@class="SearchResult_authorInfo__33M2f"]/span[2]/text()').get()
%timeit published_date = snippets.xpath('//div[@class="SearchResult_authorInfo__33M2f"]/span[2]/text()').get()

snippets = response.xpath('//*/div[@class="SearchResult_container__BnK-I"]')
for snippet in snippets:
    published_date = snippet.xpath('.//div[@class="SearchResult_authorInfo__33M2f"]/span[2]/text()').extract()
    print (published_date)

from dateutil import parser
from pytz import timezone
from tzlocal import get_localzone
tz = get_localzone()
for snippet in snippets:
    published_date = snippet.xpath('.//div[@class="SearchResult_authorInfo__33M2f"]/span[2]/text()').get()
    print (published_date)
    dt = parser.parse(published_date)
    dt = timezone(tz.key).localize(dt)
    print (dt)

from dateutil import parser
from pytz import timezone
from tzlocal import get_localzone
def parse_to_os_tz(text):
    tz = get_localzone()
    dt = parser.parse(text)
    dt = timezone(tz.key).localize(dt)
    return dt
for snippet in snippets:
    published_date = snippet.xpath('.//div[@class="SearchResult_authorInfo__33M2f"]/span[2]/text()').get()
    published_date = parse_to_os_tz(published_date)
    print (published_date)

from datetime import datetime
start_date =  datetime.utcnow()
start_date = timezone("UTC").localize(start_date)

if dt <= start_date:
    print ("yes")


%timeit headline = snippets.css('div.SearchResult_title__2OME_ > a::text').get()
%timeit headline = snippets.xpath('//*[@class="SearchResult_title__2OME_"]/a/text()').get()
%timeit headline = snippets.xpath('//div[@class="SearchResult_title__2OME_"]/a/text()').get()
%timeit headline = snippets.css('a::text').get()
%timeit headline = snippets.xpath('.//a/text()').get()

%timeit standfirst = snippets[0].css('div:nth-of-type(3) ::text').getall()
%timeit standfirst = snippets[0].xpath('.//div[3]//text()').getall()


article_url = snippets.xpath('.//a/@href').get()


########## Final shell script: ############
scrapy shell --nolog

from scrapy_splash import SplashRequest
url ='https://www.zerohedge.com/search-content?qTitleBody=bitcoin&page=0'
script="""
function main(splash, args)
    assert(splash:go(args.url))
    splash:wait((args.wait))
    splash:select('button.SimplePaginator_next__15okP'):mouse_click()
    splash:wait((args.wait))
    return splash:html()
end
"""
req = SplashRequest(url,
                    endpoint = 'execute',
                    args = {'lua_source': script, 'wait': 5.0})
fetch(req)
p2_headline = response.xpath('//*/div[@class="SearchResult_title__2OME_"]/a/text()').getall()
p2_headline

####################  End  #################



#################### Article page::
scrapy shell --nolog
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/news/2020-06-22/outlook-property-bitcoin-cryptos-and-gold-zombie-economy-part-ii'
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/crypto/three-iranian-power-plants-will-soon-be-mining-bitcoin'
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/crypto/five-reasons-sec-should-approve-bitcoin-etfs'
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/crypto/five-reasons-sec-should-approve-bitcoin-etfs'
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/markets/big-trade-hidden-under-surface'
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/political/how-democray-ends'
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/markets/complete-summary-hedge-fund-performance-sentiment-and-positioning'
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/markets/tales-cobras-windows-and-economic-promise-part-2'
scrapy shell --nolog
url = 'http://localhost:8050/render.html?url=https://www.zerohedge.com/markets/european-summit-concludes-full-summary'
fetch(url)

%timeit published_date = response.xpath('//article/header/footer/div[2]/text()').get()
%timeit published_date = response.xpath('//header/footer/div[2]/text()').getall()

from dateutil import parser
from pytz import timezone
def parse_to_utc(text):
    return timezone("UTC").localize(parser.parse(text))
print(parse_to_utc(published_date))


%timeit
author_name = response.xpath('//article/div[3]/div[1]/p[1]/a/em/text()').get()
if author_name:
    author_name = author_name.replace('Authored by ', '')
    origin_link = response.xpath('//*[@id="__next"]/div/div[5]/main/article/div[3]/div[1]/p[1]/a/@href').get()
elif not author_name:
    author_name = response.xpath('//div[@class="ContributorArticleFull_headerFooter__author__2NXEq"]/text()[2]').get()
    if not author_name:
        author_name = response.xpath('//*[@id="__next"]/div/div[5]/main/article/header/footer/div[1]/div/text()').get()
        if author_name:
            author_name = author_name.replace('by ', '')
author_name

author_twitter = response.xpath('//*[@id="__next"]/div/div[5]/main/article/div[3]/div[1]/p[105]/em/a[1]/@href').get()
author_twitter = response.xpath('//a[contains(@href,"twitter")]').getall()
author_twitter = response.xpath('//em/a[contains(@href,"twitter")]/@href')[-1].get()

author_twitter = response.xpath('//p[last()]/em/a[contains(@href,"twitter")]/@href').get()
twitter_handle = response.xpath('//p[last()]/em/a[contains(@href,"twitter")]/text()').get()

import time
%timeit article_summary = response.xpath('//article/div[3]/div[1]/ul[1]/li//text()').getall()
%timeit article_summary = response.xpath('//article/div[3]/div[1]/ul[1]/text() | //article/div[3]/div[1]/ul[1]/strong | //article/div[3]/div[1]/ul[1]/em').getall()
%timeit 
article_summary = response.xpath('//article/div[3]/div[1]/ul[1]/li/p/text() | //article/div[3]/div[1]/ul[1]/li/p/a/text() | //article/div[3]/div[1]/ul[1]/li/p/strong | //article/div[3]/div[1]/ul[1]/li/p/em').getall()


body = response.css('div.NodeContent_body__2clki.NodeBody_container__1M6aJ')
article_content = body.css('p > strong, p > a::text, li > p::text, li > p > a::text, li > p > strong, li > p > em, li > p > span::text, p > u, h1, h2, h3, h4, h5, p::text').getall()



for element in article_content:
    if (author_name in element) or (element in article_summary) or ('<strong>Summary </strong>' in element):
        article_content.remove(element)
article_content

for element in article_content:
    if element in article_summary:
        article_content.remove(element)
    if author_name in element:
        article_content.remove(element)
    if '<strong>Summary </strong>' in element:
        article_content.remove(element)
article_content

article_content = [x for x in article_content if x not in article_summary]
article_content = [x for x in article_content if author_name not in x]
article_content = [x for x in article_content if '<strong>Summary </strong>' not in x]
article_content

response.xpath
article_content = response.xpath(')





# ZH comments are local tz, articles are UTC.
comments = response.xpath('talk-slot-stream').get()
comment_date = response.xpath('//*[@class="CommentTimestamp__timestamp___2Ejbf talk-comment-timestamp TimeAgo__timeago___3aHze talk-comment-timeago"]/text()').get()





## Work sheet:

script = """
function main(splash, args)
    assert(splash:go(args.url))
    splash:select('button.SimplePaginator_next__15okP'):mouse_click()
    splash:wait(5.0) 
    return splash:html()
end
"""

script="""
function main(splash)
    splash:wait(5)
    splash:runjs('document.querySelector("button.SimplePaginator_next__15okP").submit()')
    splash:wait(5)
    return {
        html = splash:html(),
    }
end
"""

script="""
function main(splash)
    assert(splash:go("https://www.zerohedge.com/search-content?qTitleBody=bitcoin&page=0"))
    splash:wait(5)
    splash:runjs('document.querySelector("button.SimplePaginator_next__15okP").submit()')
    splash:wait(5)
    return {
        html = splash:html(),
    }
end
"""

script="""
function main(splash)
    assert(splash:go("https://www.zerohedge.com/search-content?qTitleBody=bitcoin&page=0"))
    splash:wait(5.0)
    splash:select('button.SimplePaginator_next__15okP'):mouse_click()
    splash:wait(5.0) 
    return splash:html()
end
"""

script = """
    function main(splash)
        assert(splash:go(splash.args.url))
        splash:wait(5)
        local element = splash:select('#__next > div > div > div.content > main > div > span > div > button')
        local bounds = element:bounds()
        element:mouse_click{x=bounds.width/2, y=bounds.height/2}
        return splash:html()
    end
"""

function main(splash)
    assert(splash:go(splash.args.url))
    splash:wait(5)
    local element = splash:select('#__next > div > div > div.content > main > div > span > div > button')
    local bounds = element:bounds()
    element:mouse_click{x=bounds.width/2, y=bounds.height/2}
    return splash:html()
end

document.querySelector("#__next > div > div > div.content > main > div > span > div > button")

import time
%timeit next_button = response.css('#__next > div > div > div.content > main > div > span > div > button').get()
%timeit next_button = response.css('main > div > span > div > button').get()

scrapy shell --nolog

from scrapy_splash import SplashRequest
url ='https://www.zerohedge.com/search-content?qTitleBody=bitcoin&page=0'

req = SplashRequest(url,
                    endpoint = 'execute',
                    args = {'lua_source': script, 'timeout': 5})

req = SplashRequest(url,
                    endpoint = 'execute',
                    args = {'lua_source': script})
fetch(req)